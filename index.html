<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Rahim Entezari</title>
  
  <meta name="author" content="Rahim Entezari">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Rahim Entezari</name>
              </p>
              <p>I am a PhD studet at Institute for Technical Informatics, TU Graz and Complexity Science Hub, Vienna. I try to undertand how/why deep learning works.
              </p>
              <p>
	        Before that, I did my master in AI at IUST and bachelor in computer engineering at AUT, Iran.
              </p>
              <p style="text-align:center">
                <a href="mailto:entezari@tugraz.at">Email</a> &nbsp/&nbsp
                <a href="data/cv_7.pdf">CV</a> &nbsp/&nbsp
<!--                 <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=CmTeX7kAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/rahiment">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/rahimentezari/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/rahim_pic.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/rahim_pic.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
		
	</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul style="list-style-type:circle">
		  <li> <b>October 2021: </b>I am invited to present our work on <a href="https://arxiv.org/abs/2110.06296">Permutation Invariance</a> at <a href="https://research.google/locations/montreal/">Google, Montreal</a> Sparsity group.</li>  
		  <li> <b>October 2021: </b>New <a href="https://arxiv.org/abs/2110.06296">preprint</a>is out. For a short summary see <a href="https://twitter.com/rahiment/status/1448459166675259395">this Twitter thread.</a>  </li>  			
		  <li> <b>July 2021: </b>One <a href="data/34_CameraReady_OPPO21.pdf">paper</a>. is accepted as  <b>spotlight </b> at <a href="https://sites.google.com/view/icml2021oppo/home?authuser=0">ICML 2021 Overparameterization workshop. </a> </li>  			
		  <li> <b>June 2021: </b>Two papers are accepted at <a href="https://sites.google.com/view/sparsity-workshop-2021/">Sparsity in Neural Networks workshop</a> </li>  			
		  <li> <b>Semptember 2020: </b>I achieved <a href="https://cloud.google.com/startup/">Google Startup Program</a> credit worth of 100K USD. </li> 
		  <li> <b>March 2020: </b>One <a href="https://arxiv.org/pdf/1909.10364.pdf">paper</a>. is accepted as  <b>Oral</b> at <a href="https://sensysml.github.io/sensysml2020/index">SenSys-ML workshop. </a> </li>  			

		</ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
		
		
		
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in deep learning, optimization/generalization and sparsity. Much of my research is about understaning deep learning phenomena. 
<!-- 		      Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hypernerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/2_basin_cartoon3.png' width="160">
              </div>
              <script type="text/javascript">
                function hypernerf_start() {
                  document.getElementById('hypernerf_image').style.opacity = "1";
                }

                function hypernerf_stop() {
                  document.getElementById('hypernerf_image').style.opacity = "0";
                }
                hypernerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
<!--               <a href="https://hypernerf.github.io/"> -->
                <papertitle>The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks</papertitle>
              </a>
              <br>
							<strong>Rahim Entezari</strong>,			
		    					<a href="https://haniesedghi.com/">Hanie Sedghi</a>,
							<a href="http://www.olgasaukh.com">Olga Saukh</a>, 
							<a href="https://www.neyshabur.net/">Behnam Neyshabur</a>,

		
              <br>
              <em>ICLR submission</em>, 2021 
              <br>
             <a href="https://twitter.com/rahiment/status/1448459166675259395">Twitter</a>
              /
              <a href="https://arxiv.org/abs/2110.06296">arXiv</a>
              <p></p
              <p>We conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them.</p>
            </td>
          </tr> 
	
	
<!-- 	 -->
<!-- 	 -->
<!-- 	 -->
<!-- 	 -->
	          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hypernerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/OPPO_pic.png' width="160">
              </div>
              <script type="text/javascript">
                function hypernerf_start() {
                  document.getElementById('hypernerf_image').style.opacity = "1";
                }

                function hypernerf_stop() {
                  document.getElementById('hypernerf_image').style.opacity = "0";
                }
                hypernerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Understanding the effect of sparsity on neural networks robustness</papertitle>
              </a>
              <br>
							Lukas Timple</sup>*</sup>,
							<strong>Rahim Entezari</strong></sup>*</sup>,			
		    					<a href="https://haniesedghi.com/">Hanie Sedghi</a>,
							<a href="https://www.neyshabur.net/">Behnam Neyshabur</a>,
							<a href="http://www.olgasaukh.com">Olga Saukh</a>, 
							

              <br>
              <em>ICML Overparameterization workshop</em>, 2021 
              <br>
             <a href="data/34_CameraReady_OPPO21.pdf">Paper</a>
<!--               / -->
<!--               <a href="https://arxiv.org/abs/2110.06296">arXiv</a> -->
              <p></p>
	      <p>We show that, up to a certain sparsity achieved by increasing network width and depth while keeping the network capacity fixed, sparsified networks consistently match and often outperform their initially dense versions</p>
            </td>
          </tr> 
	
	
	
	
	
	
        </tbody></table>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
